{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069f78c1",
   "metadata": {},
   "source": [
    "### RAG pipelines - Data Imgestion to vector DB pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ce483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec14e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pdf\n",
      "[PosixPath('../data/pdf/objectdetection.pdf'), PosixPath('../data/pdf/embeddings.pdf'), PosixPath('../data/pdf/attention.pdf'), PosixPath('../data/pdf/proposal.pdf')]\n",
      "\n",
      " Processing objectdetection.pdf\n",
      "\n",
      " Processed objectdetection.pdf\n",
      "\n",
      " Processing embeddings.pdf\n",
      "\n",
      " Processed embeddings.pdf\n",
      "\n",
      " Processing attention.pdf\n",
      "\n",
      " Processed attention.pdf\n",
      "\n",
      " Processing proposal.pdf\n",
      "\n",
      " Processed proposal.pdf\n",
      "Processed total 4\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all the pdf files\"\"\"\n",
    "    all_documents =[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    print(pdf_dir)\n",
    "    \n",
    "    #Find all the pdf files\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(pdf_files)\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n Processing {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file']= pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "                \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"\\n Processed {pdf_file.name}\")   \n",
    "        except Exception as e:\n",
    "            print(f\" Error occured while processing {pdf_file.name}, {e}\")\n",
    "            \n",
    "    print(f\"Processed total {len(pdf_files)}\")\n",
    "    return all_documents\n",
    "                \n",
    "    \n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "661a82c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy. \\n \\nTitle: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures. \\n \\nTitle: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Implemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment. \\n \\nTitle: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Fuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010b3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split 48 into 52 chunks\n",
      "\n",
      "Example chunk\n",
      "content: Title: A Comparative Study of CNN-Based Object Detection Models \n",
      " \n",
      "Abstract: \n",
      "This research analyzes single-stage and two-stage object detection models using a benchmark \n",
      "image dataset. \n",
      " \n",
      "Introductio...\n",
      "content: {'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"SPlit documents into chunks for better performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs= text_splitter.split_documents(documents)\n",
    "    print(f\" split {len(documents)} into {len(split_docs)} chunks\")\n",
    "    \n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk\")\n",
    "        print((f\"content: {split_docs[0].page_content[:200]}...\"))\n",
    "        print((f\"content: {split_docs[0].metadata}\"))\n",
    "        \n",
    "    return split_docs\n",
    "        \n",
    "chunks= split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639020d",
   "metadata": {},
   "source": [
    "### Embedding and vector store in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af4470f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyamshivam/Documents/projects/rag-agent-langchain/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a1d7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|| 103/103 [00:00<00:00, 1400.66it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handle document embedding generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name:str =\"all-MiniLM-L6-v2\"):\n",
    "        \n",
    "        \"\"\"Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model name for sentence embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model() # going to load model _ for protected function\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            \n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error:{e}\")\n",
    "            raise   \n",
    "        \n",
    "    def generate_embeddings(self, texts:List[str])->np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of text\"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embedding for{len(texts)} texts...\")\n",
    "        embedding = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embedding.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embedding_dimension(self):\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not found\")\n",
    "        \n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    \n",
    "### initialize the Embedding Manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b29dd",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb5af25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing document in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x14fb09e80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore():\n",
    "    \n",
    "    def __init__(self, collection_name:str= 'pdf_documents', persistent_directory:str = '../data/vector_store'):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection name: Name of the chromaDB collection\n",
    "            persistent_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client= None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "        \n",
    "        \"\"\"Initializing the chroma DB client and creating collection and persistent directory\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            \n",
    "            self.client = chromadb.PersistentClient(path= self.persistent_directory)\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"description\":\"PDF docuemnt embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing document in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while initializing Vector store. error: {e}\")\n",
    "            raise \n",
    "        \n",
    "    def add_document(self, documents: List[Any], embedding: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of langchain document\n",
    "            embeddings: Corresponding embeddings for the document\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(documents != len(embeddings)):\n",
    "            raise ValueError(\"Number of document should match the number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents in the vector store\")\n",
    "        \n",
    "        # prepare the data for ChromaDB\n",
    "        \n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # generate uuid for document\n",
    "            doc_id = f\"doc_{uuid.v4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            print(f\"ids appending: {ids}\")\n",
    "            \n",
    "            # prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata['content_length']= len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # document context \n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            #embeddings\n",
    "            embeddings_list.append(embeddings.tolist())\n",
    "            \n",
    "            # try to add to the collection\n",
    "            try:\n",
    "                self.collect.add(\n",
    "                    ids=ids,\n",
    "                    metadatas=embeddings_list,\n",
    "                    embeddings = embeddings,\n",
    "                    documents = documents_text\n",
    "                )\n",
    "                \n",
    "                print(f\"Succesfully added {len(documents)} documents to the vector store\")\n",
    "                print(f\"Total number of document in collection: {self.collection.count()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Exception occured while adding document to vector store. Error: {e}\")\n",
    "                raise\n",
    "            \n",
    "            \n",
    "vector_store = VectorStore()\n",
    "vector_store            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab49717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-agent-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
