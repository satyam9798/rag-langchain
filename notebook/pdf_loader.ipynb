{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069f78c1",
   "metadata": {},
   "source": [
    "### RAG pipelines - Data Imgestion to vector DB pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ce483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ec14e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pdf\n",
      "[PosixPath('../data/pdf/objectdetection.pdf'), PosixPath('../data/pdf/embeddings.pdf'), PosixPath('../data/pdf/attention.pdf'), PosixPath('../data/pdf/proposal.pdf')]\n",
      "\n",
      " Processing objectdetection.pdf\n",
      "\n",
      " Processed objectdetection.pdf\n",
      "\n",
      " Processing embeddings.pdf\n",
      "\n",
      " Processed embeddings.pdf\n",
      "\n",
      " Processing attention.pdf\n",
      "\n",
      " Processed attention.pdf\n",
      "\n",
      " Processing proposal.pdf\n",
      "\n",
      " Processed proposal.pdf\n",
      "Processed total 4\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all the pdf files\"\"\"\n",
    "    all_documents =[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    print(pdf_dir)\n",
    "    \n",
    "    #Find all the pdf files\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(pdf_files)\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n Processing {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file']= pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "                \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"\\n Processed {pdf_file.name}\")   \n",
    "        except Exception as e:\n",
    "            print(f\" Error occured while processing {pdf_file.name}, {e}\")\n",
    "            \n",
    "    print(f\"Processed total {len(pdf_files)}\")\n",
    "    return all_documents\n",
    "                \n",
    "    \n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "661a82c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy. \\n \\nTitle: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures. \\n \\nTitle: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Implemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment. \\n \\nTitle: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Fuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010b3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split 48 into 52 chunks\n",
      "\n",
      "Example chunk\n",
      "content: Title: A Comparative Study of CNN-Based Object Detection Models \n",
      " \n",
      "Abstract: \n",
      "This research analyzes single-stage and two-stage object detection models using a benchmark \n",
      "image dataset. \n",
      " \n",
      "Introductio...\n",
      "content: {'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"SPlit documents into chunks for better performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs= text_splitter.split_documents(documents)\n",
    "    print(f\" split {len(documents)} into {len(split_docs)} chunks\")\n",
    "    \n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk\")\n",
    "        print((f\"content: {split_docs[0].page_content[:200]}...\"))\n",
    "        print((f\"content: {split_docs[0].metadata}\"))\n",
    "        \n",
    "    return split_docs\n",
    "        \n",
    "chunks= split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4470f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy. \\n \\nTitle: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='image dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/objectdetection.pdf', 'file_path': '../data/pdf/objectdetection.pdf', 'total_pages': 15, 'format': 'PDF 1.4', 'title': 'objectdetection', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14, 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Comparative Study of CNN-Based Object Detection Models \\n \\nAbstract: \\nThis research analyzes single-stage and two-stage object detection models using a benchmark \\nimage dataset. \\n \\nIntroduction: \\nObject detection identifies and localizes objects within images using bounding boxes. \\n \\nMethodology: \\n \\nEvaluated Faster R-CNN (two-stage model). \\n \\nEvaluated YOLO (single-stage model). \\n \\nCompared inference speed and detection accuracy. \\n \\nResults: \\n \\nYOLO achieved faster inference. \\n \\nFaster R-CNN produced higher accuracy. \\n \\nConclusion: \\nModel selection depends on application requirements between speed and accuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Abstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks. \\n \\nTitle: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Abstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Results: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/embeddings.pdf', 'file_path': '../data/pdf/embeddings.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'embeddings', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Title: Comparative Analysis of Word Embedding Techniques \\n \\nAbstract: \\nThis paper compares Word2Vec, GloVe, and contextual embeddings for semantic similarity \\ntasks. The goal is to evaluate embedding quality using cosine similarity metrics. \\n \\nIntroduction: \\nWord embeddings map words into dense vector space representations, preserving semantic \\nrelationships. \\n \\nMethodology: \\n \\nTrained Word2Vec on a small corpus. \\n \\nEvaluated similarity scores between related word pairs. \\n \\nCompared with pre-trained contextual embeddings. \\n \\nResults: \\n \\nContextual embeddings captured polysemy better. \\n \\nStatic embeddings performed efficiently on smaller datasets. \\n \\nConclusion: \\nContextual embeddings outperform static embeddings in semantic understanding tasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Faster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures. \\n \\nTitle: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Implemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention.pdf', 'file_path': '../data/pdf/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'attention.pdf', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10, 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Title: A Lightweight Study on Attention Mechanisms in Neural Networks \\n \\nAbstract: \\nAttention mechanisms allow neural networks to dynamically focus on relevant parts of input \\ndata. This study evaluates scaled dot-product attention in sequence modeling tasks and \\nanalyzes its computational efficiency and contextual learning capabilities. \\n \\nIntroduction: \\nTraditional recurrent models struggle with long-range dependencies. Attention improves \\nrepresentation learning by assigning weights to important tokens in the input sequence. \\n \\nMethodology: \\n \\nImplemented scaled dot-product attention. \\n \\nCompared attention-based model with baseline LSTM. \\n \\nEvaluated on a small text classification dataset. \\n \\nResults: \\n \\nImproved contextual understanding. \\n \\nReduced dependency loss in longer sequences. \\n \\nFaster convergence compared to vanilla LSTM. \\n \\nConclusion: \\nAttention significantly enhances sequence modeling and forms the foundation of transformer \\narchitectures.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment. \\n \\nTitle: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='understanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Fuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m147 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/proposal.pdf', 'file_path': '../data/pdf/proposal.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9, 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Proposal for a Multi-Modal Transformer Model \\n \\nProblem Statement: \\nCurrent models struggle to efficiently combine text and image features for contextual \\nunderstanding. \\n \\nObjective: \\nDevelop a lightweight transformer model integrating visual and textual embeddings. \\n \\nProposed Method: \\n \\nUse pre-trained visual encoder for image features. \\n \\nUse transformer-based text encoder. \\n \\nFuse embeddings using cross-attention layers. \\n \\nExpected Outcomes: \\n \\nImproved contextual understanding across modalities. \\n \\nEfficient representation learning for multi-modal tasks. \\n \\nFuture Work: \\n \\nExtend to video-text tasks. \\n \\nOptimize for real-time deployment.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-agent-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
